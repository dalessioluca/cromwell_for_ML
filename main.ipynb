{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MINIMAL EXAMPLE OF JUPYTER NOTEBOOK WHICH CAN BE RUN WITH CROMWELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem is that it produces run2.html and trial_v1_movie_rec.gif in local folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT NECESSARY MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib\n",
    "#!pip install pyro-ppl --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import moviepy.editor as mpy\n",
    "import numpy as np\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" #for debugging, it decrease performance dramatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyro.__version__  -->  1.2.1\n",
      "torch.__version__ -->  1.4.0\n"
     ]
    }
   ],
   "source": [
    "from utilities import *\n",
    "from vae_model import * \n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "# Set up pyro environment\n",
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "# Check versions\n",
    "print(\"pyro.__version__  --> \",pyro.__version__)\n",
    "print(\"torch.__version__ --> \",torch.__version__)\n",
    "assert(pyro.__version__.startswith('1.2'))\n",
    "assert(torch.__version__.startswith('1.4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read two json files and combine in single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wdl.file_train': 'gs://ld-data-bucket/data/fashionmnist_train.pkl', 'wdl.file_test': 'gs://ld-data-bucket/data/fashionmnist_test.pkl', 'wdl.file_ckpt': 'gs://ld-results-bucket/ckpt/dummy.pkl', 'wdl.bucket_output': 'gs://ld-results-bucket', 'wdl.dir_output': 'trial_v1', 'wdl.notebook_name': 'main.ipynb', 'wdl.git_repo': 'https://github.com/dalessioluca/cromwell_for_ML.git', 'wdl.commit_or_branch': 'master', 'simulation': {'__comment': 'there are 3 types of runs: scratch resume pre_trained', 'type': 'scratch'}, 'architecture': {'__comment': 'parameters specifying the architecture of the model', 'dim_zwhat': 25, 'width_input_image': 28, 'ch_input_image': 1}, 'loss': {'__comment': 'parameter of the observation model', 'mse_sigma': 0.1}, 'optimizer': {'__comment': 'which optimizer to use', 'type': 'adam', 'lr': 0.001, 'betas': [0.9, 0.999], 'eps': 1e-08}, 'training': {'__comment': 'parameter of the observation model', 'EPOCHS': 1, 'TEST_FREQUENCY': 5, 'CHECKPOINT_FREQUENCY': 20, 'batch_size': 64, 'scheduler_is_active': False, 'scheduler_type': 'step_LR', 'scheduler_step_size': 100, 'scheduler_gamma': 0.75}}\n"
     ]
    }
   ],
   "source": [
    "working_dir = os.getcwd()\n",
    "params = load_json_as_dict(os.path.join(working_dir,\"parameters.json\"))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-63bd1500346c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"simulation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"scratch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CROMWELL will localize: \n",
    "# gs://ld-data-bucket/data/fashionmnist_train.pkl -> execution_dir/ld-data-bucket/data/fashionmnist_train.pkl\n",
    "# Therefore I just need to remove  \"gs://\"\n",
    "# Note that every path is in reference to the execution_dir\n",
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text  # or whatever\n",
    "\n",
    "train_file = remove_prefix(params[\"wdl.file_train\"],\"gs://\")\n",
    "test_file = remove_prefix(params[\"wdl.file_train\"],\"gs://\")\n",
    "ckpt_file = remove_prefix(params[\"wdl.file_ckpt\"],\"gs://\")\n",
    "dir_output = params[\"wdl.dir_output\"]\n",
    "\n",
    "# create output directionry if it does nto exists\n",
    "try:\n",
    "    os.mkdir(dir_output)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Save file to output dir\n",
    "json_param_file = os.path.join(dir_output, \"input.json\")\n",
    "save_dict_as_json(params,json_param_file)\n",
    "\n",
    "# checks\n",
    "assert os.path.isfile(train_file)\n",
    "assert os.path.isfile(test_file)\n",
    "if params[\"simulation\"][\"type\"] != \"scratch\":\n",
    "    assert os.path.isfile(ckpt_file)\n",
    "    \n",
    "print(train_file)\n",
    "print(test_file)\n",
    "print(dir_output)\n",
    "print(log_file)\n",
    "print(json_param_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###    \n",
    "###    \n",
    "###    \n",
    "###    if params[\"wdl.active\"]:\n",
    "###    \n",
    "###    # At this point WDL has localized the files in the cromwell directory\n",
    "###    # Therefore remove \"gs://\" from the file name\n",
    "###    # gs://ld-data-bucket/data/fashionmnist_train.pkl -> ld-data-bucket/data/fashionmnist_train.pkl\n",
    "###    \n",
    "###    FROM HERE\n",
    "###    wdl.train_data\" : \"data/fashionmnist_train.pkl\",\n",
    "###    \"wdl.test_data\n",
    "###    \n",
    "###\n",
    "###    \n",
    "###    # prepare output directory and check if exist\n",
    "###    working_dir = os.getcwd()\n",
    "###    output_dir = os.path.join(working_dir,\"results\")\n",
    "###\n",
    "###else:\n",
    "###    train_file = os.path.basename(params[\"run_ml_with_wdl.train_data\"])\n",
    "###    test_file = os.path.basename(params[\"run_ml_with_wdl.train_data\"])\n",
    "###    ckpt_file = os.path.basename(params[\"run_ml_with_wdl.checkpoint\"])\n",
    "###    \n",
    "###    # \n",
    "###    # gs://ld-data-bucket/data/fashionmnist_train.pkl -> ld-data-bucket/data/fashionmnist_train.pkl\n",
    "###    # the google bucket information is replaced by \"./\"\n",
    "###    # prepare input file and check if exist\n",
    "###    train_tmp = params[\"wdl.train_data\"].split(\"/\")\n",
    "###    test_tmp = params[\"wdl.test_data\"].split(\"/\")\n",
    "###    ckpt_tmp = params[\"wdl.checkpoint\"].split(\"/\")\n",
    "###    train_file = os.path.join(train_tmp[-2],train_tmp[-1])\n",
    "###    test_file = os.path.join(test_tmp[-2],test_tmp[-1])\n",
    "###    ckpt_file = os.path.join(ckpt_tmp[-2],ckpt_tmp[-1])\n",
    "###    \n",
    "###    # prepare output files\n",
    "###    working_dir = os.getcwd()\n",
    "###    output_dir = os.path.join(working_dir,params[\"simulation\"][\"output_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start logging some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"luca_logging: %(message)s\",\n",
    "                    filename=log_file,\n",
    "                    filemode=\"w\")\n",
    "console = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"luca_logging: %(message)s\")\n",
    "console.setFormatter(formatter)  # Use the same format for stdout.\n",
    "logging.getLogger('').addHandler(console)  # Log to stdout and a file.\n",
    "\n",
    "# Log the start time.\n",
    "logging.info(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"saving input json in output directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"get the data\")\n",
    "train_dataset = DatasetInMemory(train_file,use_cuda=torch.cuda.is_available())\n",
    "test_dataset  = DatasetInMemory(test_file,use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Instantiate model and optimizer\")\n",
    "vae = VaeClass(params)\n",
    "optimizer = instantiate_optimizer(vae, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 3 possible simulation types: scratch, resumed, pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"simulation type = \"+str(params[\"simulation\"][\"type\"]))\n",
    "\n",
    "if (params[\"simulation\"][\"type\"] == \"scratch\"):\n",
    "    \n",
    "    epoch_restart = -1\n",
    "    history_dict = {}\n",
    "    min_test_loss = 99999999\n",
    "\n",
    "elif (params[\"simulation\"][\"type\"] == \"resumed\"):\n",
    "        \n",
    "    resumed = load_info(path=params[\"simulation\"][\"path_to_file\"], \n",
    "                        load_epoch=True, \n",
    "                        load_history=True)\n",
    "    epoch_restart = resumed.epoch\n",
    "    history_dict = resumed.history_dict\n",
    "    min_test_loss = min(history_dict[\"test_loss\"])\n",
    "    \n",
    "    load_model_optimizer(path=params[\"simulation\"][\"path_to_file\"], \n",
    "                         model=vae,\n",
    "                         optimizer=optimizer)\n",
    "\n",
    "elif (params[\"simulation\"][\"type\"] == \"pretrained\"):\n",
    "       \n",
    "    epoch_restart = -1\n",
    "    history_dict = {}\n",
    "    min_test_loss = 99999999\n",
    "    \n",
    "    load_model_optimizer(path=params[\"simulation\"][\"path_to_file\"], \n",
    "                         model=vae,\n",
    "                         optimizer=None)\n",
    "    \n",
    "# instantiate the scheduler if necessary    \n",
    "if params[\"training\"][\"scheduler_is_active\"]:\n",
    "    scheduler = instantiate_scheduler(optimizer, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FREQUENCY = params[\"training\"][\"TEST_FREQUENCY\"]\n",
    "CHECKPOINT_FREQUENCY = params[\"training\"][\"CHECKPOINT_FREQUENCY\"]\n",
    "NUM_EPOCHS = params[\"training\"][\"EPOCHS\"]\n",
    "BATCH_SIZE = params[\"training\"][\"batch_size\"]\n",
    "\n",
    "logging.info(\"start training -> \"+datetime.now().strftime('%Y-%m-%d %H:%M:%S')) \n",
    "try:\n",
    "    for delta_epoch in range(1,NUM_EPOCHS+1):\n",
    "        epoch = delta_epoch+epoch_restart\n",
    "        vae.train()   \n",
    "        \n",
    "        #with torch.autograd.set_detect_anomaly(True):\n",
    "        with torch.autograd.set_detect_anomaly(False):\n",
    "            train_metrics = train_one_epoch(vae, \n",
    "                                            train_dataset, \n",
    "                                            optimizer, \n",
    "                                            BATCH_SIZE, \n",
    "                                            verbose=False, \n",
    "                                            weight_clipper=None)\n",
    "            s = pretty_print_metrics(epoch, train_metrics, is_train=True)\n",
    "            logging.info(s)\n",
    "                \n",
    "            \n",
    "            history_dict = add_named_tuple_to_dictionary(namedtuple=train_metrics, \n",
    "                                                         dictionary=history_dict,\n",
    "                                                         key_prefix=\"train_\")\n",
    "            \n",
    "        if params[\"training\"][\"scheduler_is_active\"]:\n",
    "            scheduler.step(epoch=epoch)\n",
    "        \n",
    "        if(epoch % TEST_FREQUENCY == 0):\n",
    "            vae.eval()\n",
    "            test_metrics = train_one_epoch(vae, \n",
    "                                           test_dataset, \n",
    "                                           optimizer, \n",
    "                                           BATCH_SIZE, \n",
    "                                           verbose=False, \n",
    "                                           weight_clipper=None)\n",
    "            \n",
    "            s = pretty_print_metrics(epoch, test_metrics, is_train=False)\n",
    "            logging.info(s)\n",
    "                    \n",
    "            history_dict = add_named_tuple_to_dictionary(namedtuple=test_metrics, \n",
    "                                                         dictionary=history_dict,\n",
    "                                                         key_prefix=\"test_\")\n",
    "            \n",
    "            test_loss = test_metrics[\"loss\"]\n",
    "            min_test_loss = min(min_test_loss, test_loss)\n",
    "                \n",
    "            #if((test_loss == min_test_loss) or ((epoch % CHECKPOINT_FREQUENCY) == 0)): \n",
    "            if((test_loss == min_test_loss) or ((epoch % TEST_FREQUENCY) == 0)):\n",
    "                checkpoint_file = os.path.join(output_dir, \"ckp_\"+str(epoch)+\".pkl\")\n",
    "                history_file = os.path.join(output_dir, \"history_\"+str(epoch)+\".pkl\")\n",
    "                \n",
    "                save_everything(model=vae, \n",
    "                                optimizer=optimizer, \n",
    "                                history_dict=history_dict, \n",
    "                                epoch=epoch, \n",
    "                                params_dict=params, \n",
    "                                path=checkpoint_file)\n",
    "                \n",
    "                save_dict_as_json(history_dict, path=history_file)\n",
    "                logging.info(\"saved files -> \"+checkpoint_file+\"  \"+history_file)\n",
    "                \n",
    "    logging.info(\"end training -> \"+datetime.now().strftime('%Y-%m-%d %H:%M:%S')) \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging.info(\"Keyboard interrupt.  Terminated without saving.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in history_dict.items():\n",
    "    print(k,\" -->\", history_dict[k][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plt.yscale('log')\n",
    "y_shift=0\n",
    "x_shift=0\n",
    "sign=1\n",
    "\n",
    "fontsize=10\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('REC',fontsize=fontsize)\n",
    "ax.set_ylabel('REG',fontsize=fontsize)\n",
    "\n",
    "ax.plot(np.arange(x_shift, x_shift+len(history_dict[\"train_loss\"])), sign*np.array(history_dict[\"train_loss\"])+y_shift,'-')\n",
    "ax.plot(np.arange(x_shift, x_shift+len(history_dict[\"test_loss\"])*TEST_FREQUENCY,TEST_FREQUENCY), sign*np.array(history_dict[\"test_loss\"])+y_shift, '.--')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('LOSS = - ELBO')\n",
    "ax.set_title('Training procedure')\n",
    "ax.grid(True)\n",
    "ax.legend(['train', 'test_clean', 'test_noisy'])\n",
    "\n",
    "fig.tight_layout()\n",
    "tmp_file = os.path.join(dir_output, \"loss.png\")\n",
    "fig.savefig(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot of KL vs evidence\n",
    "fontsize=20\n",
    "labelsize=20\n",
    "\n",
    "how_many = 2000\n",
    "scale= 1\n",
    "N = len(history_dict[\"train_kl\"][-how_many :])\n",
    "colors = np.arange(0.0,N,1.0)/N\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "#plt.yscale('log')\n",
    "#plt.xlim(xmin=1.0, xmax=1.5)\n",
    "ax.set_xlabel('REC',fontsize=fontsize)\n",
    "ax.set_ylabel('REG',fontsize=fontsize)\n",
    "ax.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "ax.scatter(history_dict[\"train_nll\"][-how_many :], history_dict[\"train_kl\"][-how_many :],c=colors)\n",
    "ax.plot(history_dict[\"train_nll\"][-how_many :], history_dict[\"train_kl\"][-how_many :], '-')\n",
    "ax.grid()\n",
    "#plt.xlim(xmax=2.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "tmp_file = os.path.join(dir_output, \"rec_kl_trajectory.png\")\n",
    "fig.savefig(tmp_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp_list = [291, 413, 133, 148, 1,2,3,4,5,6,7,8,9]\n",
    "reference_imgs, labels=test_dataset.load(batch_size=9, indices=tmp_list)\n",
    "metric, inference = vae.reconstruct_img(reference_imgs)\n",
    "\n",
    "reconstruction_file = os.path.join(output_dir, \"imgs_reconstructed.png\")\n",
    "reference_file = os.path.join(output_dir, \"imgs_reference.png\")\n",
    "\n",
    "imgs_ref = show_batch(reference_imgs[:],n_col=3,n_padding=4,title=\"REFERENCE\")\n",
    "imgs_ref.savefig(reference_file)\n",
    "\n",
    "imgs_rec = show_batch(inference.reconstruction, n_col=3,n_padding=4, title=\"REC_IMG\")\n",
    "imgs_rec.savefig(reconstruction_file)\n",
    "\n",
    "display(imgs_rec, imgs_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE MOVIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=\"xxx\"\n",
    "a = show_batch(inference.reconstruction[:9],n_col=3,n_padding=4,title=\"EPOCH = \"+str(epoch))\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actual loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_filenames = []\n",
    "\n",
    "for epoch in range(0,30,TEST_FREQUENCY):\n",
    "    if(epoch<10):\n",
    "        label =\"_000\"+str(epoch)\n",
    "    elif(epoch<100):\n",
    "        label = \"_00\"+str(epoch)\n",
    "    elif(epoch<1000):\n",
    "        label = \"_0\"+str(epoch)\n",
    "    elif(epoch<10000):\n",
    "        label = \"_\"+str(epoch)\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    try:\n",
    "        checkpoint_file = os.path.join(output_dir, \"ckp_\"+str(epoch)+\".pkl\")\n",
    "        _ = load_model_optimizer(path=checkpoint_file, model=vae, optimizer=None)\n",
    "        metric, inference = vae.reconstruct_img(reference_imgs)\n",
    "        tmp_fig = show_batch(inference.reconstruction[:8],n_col=4,n_padding=4,title=\"EPOCH = \"+str(epoch))\n",
    "        tmp_rec_file = os.path.join(output_dir, \"imgs_rec\"+label+\".png\")\n",
    "        rec_filenames.append(tmp_rec_file)\n",
    "        tmp_fig.savefig(tmp_rec_file, bbox_inches='tight') \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(rec_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_frame_rec(n):\n",
    "    tmp = Image(filename=rec_filenames[n])\n",
    "    return display(tmp)\n",
    "\n",
    "def show_frame_all(n):\n",
    "    c = Image(filename=rec_filenames[n])\n",
    "    return display(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make gif file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rec_file_local = \"movie_rec.gif\"\n",
    "movie_rec_file_absolute = os.path.join(output_dir, movie_rec_file_local)\n",
    "\n",
    "frame_per_second = 2\n",
    "im = mpy.ImageSequenceClip(rec_filenames, fps=frame_per_second)\n",
    "im.write_gif(movie_rec_file_local, fps=frame_per_second)\n",
    "im.write_gif(movie_rec_file_absolute, fps=frame_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<img src=\"+movie_rec_file_local+\"></img>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_rec(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(reference_imgs[:8],n_col=4,n_padding=4,title=\"REFERENCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
